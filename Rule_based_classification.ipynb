{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ4edQWiUk3wK0i4PjZGlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JennyFrost/Sentence_decomposition_microservice/blob/main/Rule_based_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-O1Ge8XMQ9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f29192c-8377-43a1-e610-2c8a4aeca6b9"
      },
      "source": [
        "!wget \"https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label\" -O train.txt\n",
        "!wget \"https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label\" -O test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-21 23:30:33--  https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label\n",
            "Resolving cogcomp.seas.upenn.edu (cogcomp.seas.upenn.edu)... 158.130.57.77\n",
            "Connecting to cogcomp.seas.upenn.edu (cogcomp.seas.upenn.edu)|158.130.57.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 335858 (328K)\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>] 327.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-05-21 23:30:33 (10.3 MB/s) - ‘train.txt’ saved [335858/335858]\n",
            "\n",
            "--2021-05-21 23:30:33--  https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label\n",
            "Resolving cogcomp.seas.upenn.edu (cogcomp.seas.upenn.edu)... 158.130.57.77\n",
            "Connecting to cogcomp.seas.upenn.edu (cogcomp.seas.upenn.edu)|158.130.57.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23354 (23K)\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]  22.81K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2021-05-21 23:30:34 (2.96 MB/s) - ‘test.txt’ saved [23354/23354]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w27jR4uvLtAf"
      },
      "source": [
        "def read_infile(infile):\n",
        "    sents, labels = [], []\n",
        "    with open(infile, \"r\", encoding=\"Windows-1251\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                continue\n",
        "            label, sent = line.split()[0], ' '.join(line.split()[1:])\n",
        "            sents.append(sent)\n",
        "            labels.append(label)\n",
        "    return sents, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGjYFftLspO"
      },
      "source": [
        "train_data, train_labels = read_infile(\"train.txt\")\n",
        "test_data, test_labels = read_infile(\"test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqYqGl4X90lM",
        "outputId": "12e27b9f-dc6f-43bb-e758-95aa39b4c597"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUudMDLZQF1X"
      },
      "source": [
        "Извлечение подлежащих-существительных и зависимых от них существительных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2HmM5umRVRr"
      },
      "source": [
        "def get_subject(doc):\n",
        "  subjects = [token for token in doc if token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass']\n",
        "  subj_dep = []\n",
        "  for subj in subjects:\n",
        "    subj_dep.extend([token.lemma_ for token in subj.subtree if token.pos_ == 'NOUN'])\n",
        "  return subj_dep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvNVSvn0PvU0"
      },
      "source": [
        "Извлечение синсетов для подлежащих и их зависимых из wordnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkW5Hz5UPuhj"
      },
      "source": [
        "def get_synsets(doc):\n",
        "  synsets = [wn.synsets(subj, pos=wn.NOUN) for subj in get_subject(doc)]  # берем все синсеты, в которых данное слово является существительным\n",
        "  return synsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trHkHfsDP-gt"
      },
      "source": [
        "Извлечение гиперонимов для данного синсета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxATh75NxiX8"
      },
      "source": [
        "def get_hypernyms(synset):\n",
        "  if synset.hypernyms() == []:\n",
        "    return None\n",
        "  else:\n",
        "    hypernyms = synset.hypernyms()\n",
        "    for hypernym in hypernyms:\n",
        "      name = hypernym.name().split('.')[0]\n",
        "      if name == 'animal':\n",
        "        label = 'ENTY:animal'\n",
        "      elif name == 'body_part':\n",
        "        label = 'ENTY:body'\n",
        "      elif name in ['production', 'creation', 'art', 'social event', 'show', 'auditory_communication', 'written_communication']:\n",
        "        label = 'ENTY:cremat'\n",
        "      elif name in ['organization', 'group', 'administrative_unit']:\n",
        "        label = 'HUM:gr'\n",
        "      elif name == 'substance':\n",
        "        label = 'ENTY:substance'\n",
        "      elif name == 'food':\n",
        "        label = 'ENTY:food'\n",
        "      elif name == 'vehicle':\n",
        "        label = 'ENTY:vehicle'\n",
        "      elif name == 'pathological_state':\n",
        "        label = 'ENTY:dismed'\n",
        "      elif name == 'plant':\n",
        "        label = 'ENTY:plant'\n",
        "      elif name == 'phone':\n",
        "        label = 'ENTY:letter'\n",
        "      elif name == 'occupation':\n",
        "        label = 'HUM:title'\n",
        "      elif name == 'person':\n",
        "        label = 'HUM:ind'\n",
        "      elif name in ['geological_formation', 'body_of_water', 'topographic_point', 'building']:\n",
        "        label = 'LOC:other'\n",
        "\n",
        "\n",
        "      else:\n",
        "        return get_hypernyms(hypernym)\n",
        "\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlBNwffcRN8S"
      },
      "source": [
        "Загрузка модели без парсера (для лемматизации всех предложений)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk9_nr7yoDtl"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHUzoubL5N9s"
      },
      "source": [
        "def normalize_sent(data):\n",
        "    if isinstance(data, list):\n",
        "        processed_sents = list(nlp.pipe(data))\n",
        "        return [normalize_sent(sent) for sent in processed_sents]\n",
        "    elif isinstance(data, str):\n",
        "        processed_sent = nlp(data)\n",
        "    else:\n",
        "        processed_sent = data\n",
        "    answer = [token.lemma_ if token.lemma_ != \"-PRON-\" else token.text.lower() for token in processed_sent]\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfx6YD_8QFuU"
      },
      "source": [
        "Классификатор по словам/словосочетаниям, которые есть в предложениях (без гиперонимов и синтаксиса)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQbdLlhiAdAy"
      },
      "source": [
        "def classify_basic(sent):\n",
        "  join = ' '.join(sent).lower()\n",
        "  if ('weigh' in sent) or ('weight' in sent):\n",
        "    label = 'NUM:weight'\n",
        "  elif (sent[:2] in (['how', 'many'], ['how', 'much'])) or (sent[1:3] in (['how', 'many'], ['how', 'much'])):\n",
        "    if [word for word in sent if word in ['money', 'spend', 'cost', 'charge', 'tax', 'fine', 'worth']]:\n",
        "      label = 'NUM:money'\n",
        "    else:\n",
        "      label = 'NUM:count'\n",
        "  elif [word for word in join if word in ['population', 'number', 'amount', 'death toll', 'reactivity', 'latitude', 'longitude', 'hourly rate', 'score', 'statistics']] or 'how often' in join:\n",
        "    label = 'NUM:other'\n",
        "  elif ('how long' in join) or ('how old' in join) or ('average age' in join) or ('average time' in join) or ('life span'in join) or ('life expectancy'in join):\n",
        "    label = 'NUM:period'\n",
        "  elif ((sent[0] == 'how') and (sent[1] in ['far', 'long', 'tall', 'high', 'wide', 'deep'])) or ('length' in sent):\n",
        "    label = 'NUM:dist'\n",
        "  elif ('how hot' in join) or ('how cold' in join) or ('temperature' in sent):\n",
        "    label = 'NUM:temp'\n",
        "  elif (sent[:2] == ['how', 'fast']) or ('speed' in sent):\n",
        "    label = 'NUM:speed'\n",
        "  elif (sent[0] == 'how' and sent[1] in ['large', 'big']) or (sent[0] == 'what' and 'size' in sent):\n",
        "    label = 'NUM:volsize'\n",
        "  elif (sent[0] == 'how'):\n",
        "    label = 'DESC:manner'\n",
        "  elif (sent[0] == 'when') or ('date' in sent) or ('birthday' in sent) or ('what year' in join) or ('what is the year' in join):\n",
        "    label = 'NUM:date'\n",
        "  elif [word for word in sent if word in ['abbreviation', 'abbreviate', 'acronym']]:\n",
        "    label = 'ABBR:abb'\n",
        "  elif ('stand for' in join) or ('full form of' in join) or ((('what be' in join) or ('mean' in sent)) and (re.match(r'[A-Z]+', join))):\n",
        "    label = 'ABBR:exp'\n",
        "  elif ('come from' in join) or ('what be the origin of' in join) or ('what be the difference between' in join) or ('what be the history of' in join) or ('what be the use of' in join):\n",
        "    label = 'DESC:desc'\n",
        "  elif ('definition of' in join) or ('what be the meaning of' in join) or ('define' in sent) or (('what does' in join) and ('mean' in sent)) or (sent[0] == 'what' and sent[1] == 'be' and len(sent) in [4, 5, 6]):\n",
        "    label = 'DESC:def'\n",
        "  elif 'color' in sent:\n",
        "    label = 'ENTY:color'\n",
        "  elif (sent[0] == 'why') or (sent[:2] == ['what', 'cause']) or (sent[:2] == ['what', 'make']) or (sent[-2:] == ['famous', 'for']) or (sent[-2:] == ['know', 'for']) or ('reason' in sent) or ('purpose' in sent) or ('function' in sent):\n",
        "    label = 'DESC:reason'\n",
        "  elif 'plant' in sent:\n",
        "    label = 'ENTY:plant'\n",
        "  elif ('animal' in sent) or ('bird' in sent):\n",
        "    label = 'ENTY:animal'\n",
        "  elif (join[-2:] in ['make from', 'make of', 'consist of']) or ('substance' in sent):\n",
        "    label = 'ENTY:substance'\n",
        "  elif [word for word in sent if word in ['word', 'noun', 'verb', 'adjective', 'adverb']]:\n",
        "    label = 'ENTY:word'\n",
        "  elif (sent[0] == 'what') and (sent[1] == 'be') and (('fear' in sent) or ('disease' in sent) or ('illness' in sent)):\n",
        "    label = 'ENTY:dismed'\n",
        "  elif ('sport' in sent) or ('game' in sent) or ('play' in sent):\n",
        "    label = 'ENTY:sport'\n",
        "  elif ('food' in sent) or ('taste' in sent) or ('eat' in sent):\n",
        "    label = 'ENTY:food'\n",
        "  elif 'language' in sent:\n",
        "    label = 'ENTY:lang'\n",
        "  elif 'instrument' in sent:\n",
        "    label = 'ENTY:instru'\n",
        "  elif 'letter' in sent:\n",
        "    label = 'ENTY:letter'\n",
        "  elif (sent[:2] == ['what', 'money']) or ('currency' in sent):\n",
        "    label = 'ENTY:currency'\n",
        "  elif [word for word in sent if word in ['title', 'profession', 'job', 'occupation']] or (sent[-1] == 'do') or (sent[-4:] == ['do', 'for', 'a', 'living']):\n",
        "    label = 'HUM:title'\n",
        "  elif re.match(r'What( be)?( the)?( \\w+( \\w+)?( \\'s)?)?( first| second| last| real)?( name| nickname)( of)?', join):\n",
        "    label = 'HUM:ind'\n",
        "  elif [word for word in sent if word in ['mountain', 'Mountain', 'Mountains', 'mount', 'Mount', 'peak']]:\n",
        "    label = 'LOC:mount'\n",
        "  elif ('city' in sent) or ('town' in sent) or ('capital' in sent):\n",
        "    label = 'LOC:city'\n",
        "  elif 'country' in sent:\n",
        "    label = 'LOC:country'\n",
        "  elif 'state' in sent:\n",
        "    label = 'LOC:state'\n",
        "  elif ('where' in sent) or ('address' in sent) or ('location' in sent):\n",
        "    label = 'LOC:other'\n",
        "  elif [word for word in sent if word in ['symbol', 'sign', 'mark', 'trademark']]:\n",
        "    label = 'ENTY:symbol'\n",
        "  elif ('call' in sent) or ('term' in sent) or ('translate' in sent) or ('how do you say' in join) or ('known as' in join) or ('what do you call' in join) or (re.match(r'what be the \\w+ for ', join)):\n",
        "    label = 'ENTY:termeq'\n",
        "  elif (sent[0] == 'what') and ([word for word in sent if word in ['way', 'technique', 'method', 'procedure', 'formula', 'measure', 'principle', 'stroke']]):\n",
        "    label = 'ENTY:techmeth'\n",
        "  elif ([word.lower() for word in sent if word in ['war', 'battle', 'rite', 'event', 'disaster', 'tragedy', 'holiday', 'meeting', 'project', 'revolt', 'phenomenon', 'age', 'program', 'occurence', 'hurricane', 'incident', 'trial',\n",
        "                                                   'happen', 'occur', 'follow', 'organize', 'befall', 'celebrate']] or ('take place' in join)):\n",
        "    label = 'ENTY:event'\n",
        "  elif [word for word in sent if word in ['probability', 'chance', 'odd', 'rating', 'percent', 'percentage', 'fraction', 'ratio', 'rate']]:\n",
        "    label = 'NUM:perc'\n",
        "  elif 'what chapter' in join:\n",
        "    label = 'NUM:ord'\n",
        "  elif [word for word in join if word in ['phone number', 'telephone number', 'code']]:\n",
        "    label = 'NUM:code'\n",
        "  elif (sent[0] == 'who') and (sent[1] == 'be') and (sent[2][0].isupper() or sent[3][0].isupper()):\n",
        "    label = 'HUM:desc'\n",
        "\n",
        "  else:\n",
        "    label = 0\n",
        "\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qR7_ehtjUkt",
        "outputId": "92838bfb-b05b-4bcf-a086-06349ba5bf9e"
      },
      "source": [
        "# train_data_lemmatized = normalize_sent(train_data)\n",
        "test_data_lemmatized = normalize_sent(test_data)\n",
        "test_data_lemmatized[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'far', 'be', 'it', 'from', 'Denver', 'to', 'aspen', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vw2aZFHQPYU"
      },
      "source": [
        "Базовая классфикация по словам и выделение предложений для дальнейшего анализа"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kISRUkevlS3P",
        "outputId": "eb5ae9ea-994b-4b36-c753-5fc8819eb105"
      },
      "source": [
        "labels_classified = []\n",
        "sents_for_further_analysis_numbers = []\n",
        "sents_for_further_analysis = []\n",
        "for i, sent in enumerate(test_data_lemmatized):\n",
        "  if classify_basic(sent) != 0:\n",
        "    label = classify_basic(sent)\n",
        "    labels_classified.append((i, classify_basic(sent)))\n",
        "  else:\n",
        "    sents_for_further_analysis_numbers.append(i)\n",
        "\n",
        "for num in sents_for_further_analysis_numbers:\n",
        "  for i, sent in enumerate(test_data):\n",
        "    if i == num:\n",
        "      sents_for_further_analysis.append((num, sent))\n",
        "\n",
        "\n",
        "\n",
        "# print(*labels_classified[:10], sep='\\n')\n",
        "# print(*sents_for_further_analysis[:5], sep='\\n')\n",
        "# print(*sents_for_further_analysis_numbers[:5])\n",
        "\n",
        "for sent in sents_for_further_analysis[:10]:\n",
        "  print(sent)\n",
        "print(len(sents_for_further_analysis))\n",
        "print(len(labels_classified))\n",
        "print(len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 'What county is Modesto , California in ?')\n",
            "(6, 'George Bush purchased a small interest in which baseball team ?')\n",
            "(7, \"What is Australia 's national flower ?\")\n",
            "(11, \"What person 's head is on a dime ?\")\n",
            "(13, 'Who was the first man to fly across the Pacific Ocean ?')\n",
            "(16, 'What metal has the highest melting point ?')\n",
            "(17, 'Who developed the vaccination against polio ?')\n",
            "(20, 'Who was the first American to walk in space ?')\n",
            "(22, 'What river in the US is known as the Big Muddy ?')\n",
            "(25, 'Who developed the Macintosh computer ?')\n",
            "151\n",
            "349\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2uzE6_xQbJA"
      },
      "source": [
        "Загрузка модели с парсером для анализа оставшихся предложений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oGnwYmE389K"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "docs = list(nlp.pipe([elem[1] for elem in sents_for_further_analysis])) #sents_for_further_analysis - предложения, оставшиеся после того, как убрали предложения, которые классифицировала функция classify_basic, и функция get_hypernyms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k08LbauBoCK",
        "outputId": "b7e4a476-a1be-4999-da1b-fccafa2d8ab1"
      },
      "source": [
        "processed_sents = list(zip(sents_for_further_analysis_numbers, docs))\n",
        "print(processed_sents[:10])\n",
        "print(len(processed_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, What county is Modesto , California in ?), (6, George Bush purchased a small interest in which baseball team ?), (7, What is Australia 's national flower ?), (11, What person 's head is on a dime ?), (13, Who was the first man to fly across the Pacific Ocean ?), (16, What metal has the highest melting point ?), (17, Who developed the vaccination against polio ?), (20, Who was the first American to walk in space ?), (22, What river in the US is known as the Big Muddy ?), (25, Who developed the Macintosh computer ?)]\n",
            "151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlBPilSYQyiR"
      },
      "source": [
        "Классификация с использованием гиперонимов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vcSzK9byB2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce77bc17-0cc8-47f0-a687-b894498fec2d"
      },
      "source": [
        "sents_for_extra_classifier = []\n",
        "for num, doc in processed_sents:\n",
        "  for synsets in get_synsets(doc):\n",
        "    for synset in synsets:\n",
        "      if get_hypernyms(synset):\n",
        "        # if num not in [elem[0] for elem in labels_classified]:\n",
        "          labels_classified.append((num, get_hypernyms(synset)))\n",
        "          break\n",
        "    if num in [elem[0] for elem in labels_classified]:\n",
        "      break\n",
        "  if num not in [elem[0] for elem in labels_classified]:\n",
        "    sents_for_extra_classifier.append((num, doc))\n",
        "\n",
        "print(len(labels_classified))\n",
        "len(sents_for_extra_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtRKPZ0uR2kg"
      },
      "source": [
        "Дополнительный классификатор с использованием синтаксиса для оставшихся предложений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNqEOPj-xSpp"
      },
      "source": [
        "def classify_extra(doc):\n",
        "  if (doc[0].text in ['Who', 'Name']) or ((doc[0].text in ['What', 'Which']) and (doc[0].dep_ == 'det') and (doc[0].head.dep_ == 'dobj')):\n",
        "    label = 'HUM:ind'\n",
        "  elif (doc[0].text == 'What' and doc[1].lemma_ == 'do'):\n",
        "    label = 'DESC:desc'\n",
        "  else:\n",
        "    label = 'ENTY:other'\n",
        "\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVzn44cV47mu",
        "outputId": "ab2bffb5-c575-420f-bce8-d93b475dc9ef"
      },
      "source": [
        "for num, doc in sents_for_extra_classifier:\n",
        "  if num not in [elem[0] for elem in labels_classified]:\n",
        "    labels_classified.append((num, classify_extra(doc)))\n",
        "print(len(labels_classified))\n",
        "labels_classified[:10]\n",
        "# sents_for_extra_classifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'NUM:dist'),\n",
              " (2, 'HUM:desc'),\n",
              " (3, 'DESC:def'),\n",
              " (4, 'NUM:date'),\n",
              " (5, 'NUM:dist'),\n",
              " (8, 'DESC:reason'),\n",
              " (9, 'DESC:def'),\n",
              " (10, 'LOC:city'),\n",
              " (12, 'NUM:weight'),\n",
              " (14, 'NUM:date')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz5wWllTSJhL"
      },
      "source": [
        "Получение меток в том порядке, в котором они шли в test_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blVGc-uq3G2J",
        "outputId": "8b0a9682-f944-4fe2-e2c3-b77582a18dbd"
      },
      "source": [
        "labels_classified = sorted(labels_classified)\n",
        "len(labels_classified)\n",
        "labels_classified = [elem[1] for elem in labels_classified]\n",
        "labels_classified[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NUM:dist',\n",
              " 'ENTY:other',\n",
              " 'HUM:desc',\n",
              " 'DESC:def',\n",
              " 'NUM:date',\n",
              " 'NUM:dist',\n",
              " 'ENTY:other',\n",
              " 'ENTY:plant',\n",
              " 'DESC:reason',\n",
              " 'DESC:def']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li2uaV2gSXCO"
      },
      "source": [
        "Измерение accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SeRBoWU6L4k",
        "outputId": "28e6c724-06db-4de7-af33-9ba7ec049c0a"
      },
      "source": [
        "correct = 0\n",
        "for i, label in enumerate(labels_classified):\n",
        "  if label == test_labels[i]:\n",
        "    correct += 1\n",
        "\n",
        "accuracy = correct / len(test_labels) * 100\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}